{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890e31f1",
   "metadata": {},
   "source": [
    "### Intial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6216dc56",
   "metadata": {},
   "source": [
    "Installation of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90852f1a",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autogen-agentchat\n",
      "  Downloading autogen_agentchat-0.5.6-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting autogen-ext[openai]\n",
      "  Downloading autogen_ext-0.5.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting autogen-core==0.5.6 (from autogen-agentchat)\n",
      "  Downloading autogen_core-0.5.6-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting jsonref~=1.1.0 (from autogen-core==0.5.6->autogen-agentchat)\n",
      "  Using cached jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting opentelemetry-api>=1.27.0 (from autogen-core==0.5.6->autogen-agentchat)\n",
      "  Downloading opentelemetry_api-1.32.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pillow>=11.0.0 (from autogen-core==0.5.6->autogen-agentchat)\n",
      "  Downloading pillow-11.2.1-cp311-cp311-win_amd64.whl.metadata (9.1 kB)\n",
      "Collecting protobuf~=5.29.3 (from autogen-core==0.5.6->autogen-agentchat)\n",
      "  Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting pydantic<3.0.0,>=2.10.0 (from autogen-core==0.5.6->autogen-agentchat)\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "     ---------------------------------------- 0.0/66.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 66.6/66.6 kB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from autogen-core==0.5.6->autogen-agentchat) (4.11.0)\n",
      "Requirement already satisfied: aiofiles in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from autogen-ext[openai]) (22.1.0)\n",
      "Collecting openai>=1.66.5 (from autogen-ext[openai])\n",
      "  Downloading openai-1.77.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting tiktoken>=0.8.0 (from autogen-ext[openai])\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from openai>=1.66.5->autogen-ext[openai]) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from openai>=1.66.5->autogen-ext[openai]) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from openai>=1.66.5->autogen-ext[openai]) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from openai>=1.66.5->autogen-ext[openai]) (0.6.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from openai>=1.66.5->autogen-ext[openai]) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from openai>=1.66.5->autogen-ext[openai]) (4.66.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from tiktoken>=0.8.0->autogen-ext[openai]) (2024.4.16)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from tiktoken>=0.8.0->autogen-ext[openai]) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.66.5->autogen-ext[openai]) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.66.5->autogen-ext[openai]) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.66.5->autogen-ext[openai]) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.66.5->autogen-ext[openai]) (0.14.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.27.0->autogen-core==0.5.6->autogen-agentchat) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.27.0->autogen-core==0.5.6->autogen-agentchat) (7.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.6->autogen-agentchat) (0.6.0)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.6->autogen-agentchat)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from autogen-core==0.5.6->autogen-agentchat)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.6->autogen-agentchat)\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.8.0->autogen-ext[openai]) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.8.0->autogen-ext[openai]) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from tqdm>4->openai>=1.66.5->autogen-ext[openai]) (0.4.6)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.27.0->autogen-core==0.5.6->autogen-agentchat) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.27.0->autogen-core==0.5.6->autogen-agentchat) (3.18.1)\n",
      "Downloading autogen_agentchat-0.5.6-py3-none-any.whl (105 kB)\n",
      "   ---------------------------------------- 0.0/105.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 105.5/105.5 kB 3.1 MB/s eta 0:00:00\n",
      "Downloading autogen_core-0.5.6-py3-none-any.whl (94 kB)\n",
      "   ---------------------------------------- 0.0/94.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 94.7/94.7 kB 5.3 MB/s eta 0:00:00\n",
      "Downloading openai-1.77.0-py3-none-any.whl (662 kB)\n",
      "   ---------------------------------------- 0.0/662.0 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 317.4/662.0 kB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 662.0/662.0 kB 6.9 MB/s eta 0:00:00\n",
      "Downloading tiktoken-0.9.0-cp311-cp311-win_amd64.whl (893 kB)\n",
      "   ---------------------------------------- 0.0/893.9 kB ? eta -:--:--\n",
      "   ---------------------------- ---------- 645.1/893.9 kB 13.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 893.9/893.9 kB 14.3 MB/s eta 0:00:00\n",
      "Downloading autogen_ext-0.5.6-py3-none-any.whl (296 kB)\n",
      "   ---------------------------------------- 0.0/296.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 296.8/296.8 kB 17.9 MB/s eta 0:00:00\n",
      "Using cached jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Downloading opentelemetry_api-1.32.1-py3-none-any.whl (65 kB)\n",
      "   ---------------------------------------- 0.0/65.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 65.3/65.3 kB ? eta 0:00:00\n",
      "Downloading pillow-11.2.1-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.6/2.7 MB 13.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.4/2.7 MB 14.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.1/2.7 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 15.5 MB/s eta 0:00:00\n",
      "Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "   ---------------------------------------- 0.0/434.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 434.5/434.5 kB 26.5 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "   ---------------------------------------- 0.0/443.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 443.9/443.9 kB 27.1 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 1.0/2.0 MB 30.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.7/2.0 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 20.7 MB/s eta 0:00:00\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-extensions, protobuf, pillow, jsonref, typing-inspection, tiktoken, pydantic-core, opentelemetry-api, pydantic, openai, autogen-core, autogen-ext, autogen-agentchat\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 10.3.0\n",
      "    Uninstalling pillow-10.3.0:\n",
      "      Successfully uninstalled pillow-10.3.0\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.7.0\n",
      "    Uninstalling tiktoken-0.7.0:\n",
      "      Successfully uninstalled tiktoken-0.7.0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.18.1\n",
      "    Uninstalling pydantic_core-2.18.1:\n",
      "      Successfully uninstalled pydantic_core-2.18.1\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "    Found existing installation: opentelemetry-api 1.24.0\n",
      "    Uninstalling opentelemetry-api-1.24.0:\n",
      "      Successfully uninstalled opentelemetry-api-1.24.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.7.0\n",
      "    Uninstalling pydantic-2.7.0:\n",
      "      Successfully uninstalled pydantic-2.7.0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.20.0\n",
      "    Uninstalling openai-1.20.0:\n",
      "      Successfully uninstalled openai-1.20.0\n",
      "Successfully installed autogen-agentchat-0.5.6 autogen-core-0.5.6 autogen-ext-0.5.6 jsonref-1.1.0 openai-1.77.0 opentelemetry-api-1.32.1 pillow-11.2.1 protobuf-5.29.4 pydantic-2.11.4 pydantic-core-2.33.2 tiktoken-0.9.0 typing-extensions-4.13.2 typing-inspection-0.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-cloud-auth 0.1.3 requires pydantic<2.0, but you have pydantic 2.11.4 which is incompatible.\n",
      "googleapis-common-protos 1.63.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\n",
      "google-api-core 2.18.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\n",
      "langchain 0.3.3 requires langchain-core<0.4.0,>=0.3.10, but you have langchain-core 0.1.43 which is incompatible.\n",
      "langchain-community 0.3.2 requires langchain-core<0.4.0,>=0.3.10, but you have langchain-core 0.1.43 which is incompatible.\n",
      "langchain-community 0.3.2 requires langsmith<0.2.0,>=0.1.125, but you have langsmith 0.1.48 which is incompatible.\n",
      "langchain-community 0.3.2 requires pydantic-settings<3.0.0,>=2.4.0, but you have pydantic-settings 2.2.1 which is incompatible.\n",
      "langchain-huggingface 0.1.0 requires huggingface-hub>=0.23.0, but you have huggingface-hub 0.22.2 which is incompatible.\n",
      "langchain-huggingface 0.1.0 requires langchain-core<0.4,>=0.3.0, but you have langchain-core 0.1.43 which is incompatible.\n",
      "langchain-huggingface 0.1.0 requires tokenizers>=0.19.1, but you have tokenizers 0.15.2 which is incompatible.\n",
      "langchain-openai 0.2.2 requires langchain-core<0.4.0,>=0.3.9, but you have langchain-core 0.1.43 which is incompatible.\n",
      "langchain-text-splitters 0.3.0 requires langchain-core<0.4.0,>=0.3.0, but you have langchain-core 0.1.43 which is incompatible.\n",
      "opentelemetry-proto 1.24.0 requires protobuf<5.0,>=3.19, but you have protobuf 5.29.4 which is incompatible.\n",
      "opentelemetry-sdk 1.24.0 requires opentelemetry-api==1.24.0, but you have opentelemetry-api 1.32.1 which is incompatible.\n",
      "proto-plus 1.23.0 requires protobuf<5.0.0dev,>=3.19.0, but you have protobuf 5.29.4 which is incompatible.\n",
      "tensorflow-intel 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\n",
      "torchaudio 2.2.2 requires torch==2.2.2, but you have torch 2.2.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\viraj\\anaconda3\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\viraj\\anaconda3\\lib\\site-packages (4.39.3)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from torch) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: requests in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\viraj\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   - ------------------------------------- 41.0/991.5 kB 991.0 kB/s eta 0:00:01\n",
      "   --------------------- ------------------ 532.5/991.5 kB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 991.5/991.5 kB 9.0 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U autogen-agentchat autogen-ext[openai]\n",
    "!pip install torch transformers sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf39d394",
   "metadata": {},
   "source": [
    "API key setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babfaba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f48ea",
   "metadata": {},
   "source": [
    "### Multi Agent Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd4379d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\viraj\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n",
      "flaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.\n"
     ]
    }
   ],
   "source": [
    "from autogen.agentchat import AssistantAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61131f8",
   "metadata": {},
   "source": [
    "#### Fine tuned Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cffbb65",
   "metadata": {},
   "source": [
    "wrapper agent for autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9357aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.agentchat import AssistantAgent\n",
    "\n",
    "class LocalModelWrapperAgent(AssistantAgent):\n",
    "    def __init__(self, name: str, model_agent):\n",
    "        \"\"\"\n",
    "        name: The AutoGen agent name (e.g., \"semantic-agent\", \"contrast-agent\").\n",
    "        model_agent: Any class that implements `handle(raw_json: str) -> str`.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        self.model_agent = model_agent\n",
    "\n",
    "    def generate_reply(self, messages):\n",
    "        # Expecting the last user message to contain the JSON string\n",
    "        raw_json = messages[-1][\"content\"]\n",
    "        return self.model_agent.handle(raw_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd41daaf",
   "metadata": {},
   "source": [
    "Semantic Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195e30bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from agents.semantic_agent.agent import SemanticAgent\n",
    "\n",
    "semantic_model = SemanticAgent(model_dir=\"trusha88/t5-semantic-agent\")\n",
    "semantic_agent = LocalModelWrapperAgent(name=\"semantic-agent\", model_agent=semantic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0b07cf",
   "metadata": {},
   "source": [
    "Contrast Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f791244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from agents.contrast_agent.agent import ContrastAgent\n",
    "\n",
    "contrast_model = ContrastAgent(model_dir=\"virajns2/contrast-violation-t5\")\n",
    "contrast_agent = LocalModelWrapperAgent(name=\"contrast-agent\", model_agent=contrast_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f43bbc",
   "metadata": {},
   "source": [
    "### GPT based Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f0a6414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually Impaired Agent\n",
    "visually_impaired_agent = AssistantAgent(\n",
    "    name=\"VisuallyImpairedAgent\",\n",
    "    system_message=\"Simulate a user navigating via screen reader. Identify missing labels, improper heading structures, and ARIA attribute issues.\",\n",
    "    llm_config={\"model\": \"gpt-4\", \"temperature\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08c0022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motor-Impaired Agent\n",
    "motor_impaired_agent = AssistantAgent(\n",
    "    name=\"MotorImpairedAgent\",\n",
    "    system_message=\"Simulate keyboard-only navigation. Detect elements not accessible via keyboard and lack of focus indicators.\",\n",
    "    llm_config={\"model\": \"gpt-4\", \"temperature\": 0}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae86cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color-Blind Agent\n",
    "color_blind_agent = AssistantAgent(\n",
    "    name=\"ColorBlindAgent\",\n",
    "    system_message=\"Analyze color usage for accessibility. Identify issues with color contrast and reliance on color cues.\",\n",
    "    llm_config={\"model\": \"gpt-4\", \"temperature\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c04be84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixing_agent = AssistantAgent(\n",
    "    name=\"FixingAgent\",\n",
    "    system_message=\"Provide code-level fixes for identified accessibility issues based on best practices and guidelines, based on the problems stated by everyone\",\n",
    "    llm_config={\"model\": \"gpt-4\", \"temperature\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48901926",
   "metadata": {},
   "source": [
    "Configure Agent Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b797de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.agentchat import UserProxyAgent, GroupChat, GroupChatManager\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"UserProxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=3,\n",
    "    code_execution_config={\"use_docker\": False}\n",
    ")\n",
    "\n",
    "group_chat = GroupChat(\n",
    "    agents=[user_proxy, semantic_agent, contrast_agent, visually_impaired_agent, motor_impaired_agent, color_blind_agent, fixing_agent],\n",
    "    messages=[],\n",
    "    max_round=5\n",
    ")\n",
    "\n",
    "manager = GroupChatManager(groupchat=group_chat, llm_config={\"model\": \"gpt-4\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa1291c",
   "metadata": {},
   "source": [
    "Run the Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a05ecc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: FETCH RESPECTIVE DATA FROM TEST SET AND PASS TO SYSTEM\n",
    "with open(\"test_data/test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ui = f.read()\n",
    "ui = \"role: link, fg: 120,120,120, bg: 255,255,255, contrast: 2.9\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0e675f",
   "metadata": {},
   "source": [
    "Feed back loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f190135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUserProxy\u001b[0m (to chat_manager):\n",
      "\n",
      "role: link, fg: 120,120,120, bg: 255,255,255, contrast: 2.9\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: ColorBlindAgent\n",
      "\u001b[0m\n",
      "\u001b[33mColorBlindAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "The color usage in this case has a contrast ratio of 2.9. According to the Web Content Accessibility Guidelines (WCAG), the minimum contrast ratio for normal text should be 4.5:1. Therefore, the contrast ratio here is too low, which could make it difficult for people with visual impairments, including color blindness, to read the text. \n",
      "\n",
      "Additionally, if there are any important cues or information conveyed solely through color, this would be inaccessible to those who are color blind or have other visual impairments. It's important to ensure that information is available through other means, such as text or patterns, in addition to color.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: FixingAgent\n",
      "\u001b[0m\n",
      "\u001b[33mFixingAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "To fix this issue, you need to adjust the color of the text or the background to increase the contrast ratio. Here's an example of how you can do this:\n",
      "\n",
      "If you're using CSS, you can adjust the color values like this:\n",
      "\n",
      "```css\n",
      "/* For the link role */\n",
      ".link-role {\n",
      "    /* Adjust the foreground color to a darker shade for better contrast */\n",
      "    color: rgb(80, 80, 80);\n",
      "    background-color: rgb(255, 255, 255);\n",
      "}\n",
      "```\n",
      "\n",
      "This will change the foreground color to a darker shade, increasing the contrast ratio to meet the WCAG guidelines. \n",
      "\n",
      "Remember, it's important to test the new color combination to ensure it meets the minimum contrast ratio of 4.5:1 for normal text. You can use online tools like the WebAIM Contrast Checker to do this.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: UserProxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is css)...\u001b[0m\n",
      "\u001b[33mUserProxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "unknown language css\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: semantic-agent\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LocalModelWrapperAgent.generate_reply() got an unexpected keyword argument 'sender'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m user_proxy\u001b[38;5;241m.\u001b[39minitiate_chat(manager, message \u001b[38;5;241m=\u001b[39m ui)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m group_chat\u001b[38;5;241m.\u001b[39mmessages:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\viraj\\anaconda3\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:1102\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[1;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         msg2send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_init_message(message, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg2send, recipient, silent\u001b[38;5;241m=\u001b[39msilent)\n\u001b[0;32m   1103\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarize_chat(\n\u001b[0;32m   1104\u001b[0m     summary_method,\n\u001b[0;32m   1105\u001b[0m     summary_args,\n\u001b[0;32m   1106\u001b[0m     recipient,\n\u001b[0;32m   1107\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[0;32m   1108\u001b[0m )\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m, recipient]:\n",
      "File \u001b[1;32mc:\\Users\\viraj\\anaconda3\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:738\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[1;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[0;32m    736\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient, is_sending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[1;32m--> 738\u001b[0m     recipient\u001b[38;5;241m.\u001b[39mreceive(message, \u001b[38;5;28mself\u001b[39m, request_reply, silent)\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    741\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    742\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\viraj\\anaconda3\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:902\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[1;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 902\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_reply(messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_messages[sender], sender\u001b[38;5;241m=\u001b[39msender)\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    904\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[1;32mc:\\Users\\viraj\\anaconda3\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:2056\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[1;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[0;32m   2054\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   2055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[1;32m-> 2056\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m reply_func(\u001b[38;5;28mself\u001b[39m, messages\u001b[38;5;241m=\u001b[39mmessages, sender\u001b[38;5;241m=\u001b[39msender, config\u001b[38;5;241m=\u001b[39mreply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   2057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[0;32m   2058\u001b[0m         log_event(\n\u001b[0;32m   2059\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2060\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func_executed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2064\u001b[0m             reply\u001b[38;5;241m=\u001b[39mreply,\n\u001b[0;32m   2065\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\viraj\\anaconda3\\Lib\\site-packages\\autogen\\agentchat\\groupchat.py:1131\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[1;34m(self, messages, sender, config)\u001b[0m\n\u001b[0;32m   1129\u001b[0m         iostream\u001b[38;5;241m.\u001b[39mprint(colored(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNext speaker: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspeaker\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m\"\u001b[39m), flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;66;03m# let the speaker speak\u001b[39;00m\n\u001b[1;32m-> 1131\u001b[0m     reply \u001b[38;5;241m=\u001b[39m speaker\u001b[38;5;241m.\u001b[39mgenerate_reply(sender\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;66;03m# let the admin agent speak if interrupted\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m groupchat\u001b[38;5;241m.\u001b[39madmin_name \u001b[38;5;129;01min\u001b[39;00m groupchat\u001b[38;5;241m.\u001b[39magent_names:\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;66;03m# admin agent is one of the participants\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: LocalModelWrapperAgent.generate_reply() got an unexpected keyword argument 'sender'"
     ]
    }
   ],
   "source": [
    "user_proxy.initiate_chat(manager, message = ui)\n",
    "\n",
    "for msg in group_chat.messages:\n",
    "    print(f\"{msg['role']}: {msg['content']}\\n\")\n",
    "    if msg['role'] != 'FixingAgent':\n",
    "        fixing_agent.receive_message(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa23e92f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
