{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbRGlG0sufi1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import List, Dict\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "\n",
        "class ImageCaptioningAgent:\n",
        "    \"\"\"AI agent that extracts image crops from viewport screenshots and\n",
        "    produces natural‑language captions for each using BLIP.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device: str | None = None):\n",
        "        \"\"\"Load BLIP model + processor and move to the chosen device.\n",
        "\n",
        "        Args:\n",
        "            device: Optional forced device string (\"cuda\", \"cpu\", \"mps\", …).\n",
        "                    Defaults to the first available CUDA device, else CPU.\n",
        "        \"\"\"\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Salesforce/blip-image-base is a base‑size vision‑language model that\n",
        "        # supports zero‑shot image captioning.\n",
        "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-base\")\n",
        "        self.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-base\")\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def preprocess(self, raw_json: str) -> List[Dict]:\n",
        "        \"\"\"Parse the incoming JSON and crop all bounding boxes.\n",
        "\n",
        "        Each **viewport** entry supplies a full‑page ``screenshot`` path and an\n",
        "        ``image_captioning`` array with ``nodeId``, ``alt`` text, and a ``bbox``.\n",
        "        For every item we load the screenshot exactly once per viewport, crop\n",
        "        the box, and stash the crop as a **PIL.Image** so that the next stage\n",
        "        can batch/loop over them.\n",
        "        \"\"\"\n",
        "        doc = json.loads(raw_json)\n",
        "\n",
        "        crops: List[Dict] = []\n",
        "        for vp in doc.get(\"viewports\", []):\n",
        "            screenshot_path: str | None = vp.get(\"screenshot\")\n",
        "            if not screenshot_path:\n",
        "                # Skip viewports that do not provide a screenshot path.\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                page_img = Image.open(screenshot_path).convert(\"RGB\")\n",
        "            except Exception as exc:  # pragma: no cover — I/O safeguard\n",
        "                raise RuntimeError(f\"Failed to open screenshot {screenshot_path}: {exc}\") from exc\n",
        "\n",
        "            for entry in vp.get(\"image_captioning\", []):\n",
        "                node_id = entry.get(\"nodeId\")\n",
        "                alt_text = entry.get(\"alt\", \"\")\n",
        "                bbox = entry.get(\"bbox\", {})\n",
        "\n",
        "                # Basic sanity ‑ fallback to 0 for any missing coordinate.\n",
        "                x, y = bbox.get(\"x\", 0), bbox.get(\"y\", 0)\n",
        "                w, h = bbox.get(\"width\", 0), bbox.get(\"height\", 0)\n",
        "\n",
        "                # Guard against degenerate boxes — BLIP will error on size 0.\n",
        "                if w <= 0 or h <= 0:\n",
        "                    continue\n",
        "\n",
        "                crop = page_img.crop((x, y, x + w, y + h))\n",
        "                crops.append({\n",
        "                    \"nodeId\": node_id,\n",
        "                    \"alt\": alt_text,\n",
        "                    \"image\": crop,\n",
        "                })\n",
        "\n",
        "        if not crops:\n",
        "            raise ValueError(\"No image crops found in the provided JSON\")\n",
        "\n",
        "        return crops\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def generate_summary(\n",
        "        self,\n",
        "        crops: List[Dict],\n",
        "        max_length: int = 20,\n",
        "        num_beams: int = 4,\n",
        "    ) -> List[tuple[str, str, str]]:\n",
        "        \"\"\"Run BLIP captioning over every crop.\n",
        "\n",
        "        Returns a list of ``(nodeId, alt, caption)`` tuples suitable for the\n",
        "        formatter in ``handle``.\n",
        "        \"\"\"\n",
        "        results: List[tuple[str, str, str]] = []\n",
        "        for item in crops:\n",
        "            encoded = self.processor(images=item[\"image\"], return_tensors=\"pt\").to(self.device)\n",
        "            out_ids = self.model.generate(\n",
        "                **encoded,\n",
        "                max_length=max_length,\n",
        "                num_beams=num_beams,\n",
        "                early_stopping=True,\n",
        "            )\n",
        "            caption = self.processor.decode(out_ids[0], skip_special_tokens=True).strip()\n",
        "            results.append((item[\"nodeId\"], item[\"alt\"], caption))\n",
        "        return results\n",
        "\n",
        "    def handle(self, raw_json: str) -> str:\n",
        "        \"\"\"Top‑level entry: JSON → captions in the requested plain‑text format.\"\"\"\n",
        "        crops = self.preprocess(raw_json)\n",
        "        triples = self.generate_summary(crops)\n",
        "\n",
        "        # Assemble the exact output phrasing required by the spec.\n",
        "        sentences: list[str] = []\n",
        "        for node_id, alt, caption in triples:\n",
        "            alt_text = alt if alt else \"no alternate text provided\"\n",
        "            sentences.append(\n",
        "                f\"For nodeId {node_id}, the alt image text is '{alt_text}', and the generated caption is '{caption}'.\"\n",
        "            )\n",
        "\n",
        "        return \" \".join(sentences)"
      ]
    }
  ]
}